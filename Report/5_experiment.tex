\section{EXPERIMENTS}

\subsection{Experiment Design}

We evaluate the hierarchical planner using the automated benchmark in \texttt{Code/benchmark.py}. The benchmark tests all $4\times 6=24$ combinations of global planners (\texttt{grid\_bfs}, \texttt{grid\_dfs}, \texttt{astar}, \texttt{dijkstra}) and local planners (\texttt{reactive\_bfs}, \texttt{reactive\_dfs}, \texttt{potential\_field}, \texttt{greedy}, \texttt{dwa}, \texttt{evolutionary}) across 7 maps and 4 obstacle counts $\{0,50,100,200\}$, totaling 672 runs.

Each run uses an agent observation radius $r=2$ (a $5\times5$ local window) and inflates observed obstacles with margin 1. Runs terminate at 2000 steps or 100 seconds; success is declared when Manhattan distance to the final goal is $\leq 3$ (waypoints use tolerance 2). When the agent fails to move or does not reduce distance to its current target for 15 steps, the system triggers global replanning from the current state to the final goal.

We report success rate (primary), executed path length, direction changes (smoothness proxy), accumulated global planning time (including replans), total wall-clock time, and steps. Results are saved to \texttt{Code/results/benchmark\_results.csv} and summarized using \texttt{Code/analyze\_results.py}.

Figure~\ref{fig:benchmark_maps} shows representative benchmark maps (white: free space, black: obstacles).

\begin{figure}[ht]
\centering
\captionsetup{font=small}
\captionsetup[subfigure]{font=footnotesize}

\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[width=\linewidth]{images/1.png}
\caption{Map 1}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[width=\linewidth]{images/2.png}
\caption{Map 2}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[width=\linewidth]{images/3.png}
\caption{Map 3}
\end{subfigure}

\vspace{0.4em}

\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[width=\linewidth]{images/4.png}
\caption{Map 4}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[width=\linewidth]{images/5.png}
\caption{Map 5}
\end{subfigure}
\begin{subfigure}{0.2\textwidth}
\centering
\includegraphics[width=\linewidth]{images/6.png}
\caption{Map 6}
\end{subfigure}

\caption{Six benchmark maps used for evaluation.}
\label{fig:benchmark_maps}
\end{figure}

\subsection{Benchmark Results}

The full benchmark contains 672 test configurations (7 maps $\times$ 4 obstacle settings $\times$ 24 algorithm combinations). Overall, 416/672 runs succeeded, corresponding to a 61.9\% success rate.

For successful runs, the average path length is 286.14 $\pm$ 187.46 cells, with 86.64 $\pm$ 149.21 direction changes, and an average execution time of 6.9900 $\pm$ 6.5079 seconds (accumulated global planning time: 2.3969 $\pm$ 4.6303 seconds).

\subsubsection{Success Rate by Algorithm Combination}

Figure~\ref{fig:success_rates} ranks all 24 planner combinations by success rate (each entry aggregates 7 maps $\times$ 4 obstacle settings, i.e., 28 runs per combination).

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{../Code/results/success_rates.png}
\caption{Success rate by global+local algorithm combination (computed from \texttt{benchmark\_results.csv}).}
\label{fig:success_rates}
\end{figure}

The most reliable combinations are \texttt{astar} + \texttt{dwa} and \texttt{dijkstra} + \texttt{evolutionary} (96.4\%), with \texttt{grid\_bfs} + \texttt{potential\_field} matching the same success rate. \texttt{potential\_field}, \texttt{dwa}, and \texttt{evolutionary} consistently outperform \texttt{reactive\_bfs} and \texttt{greedy}, while \texttt{grid\_dfs} as the global planner reduces success rates across nearly all local methods. In contrast, \texttt{reactive\_dfs} fails across all global planners (0/28), indicating that its depth-first local behavior is not robust to dynamic obstacles and the benchmark replanning trigger.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{../Code/results/metrics_comparison.png}
\caption{Comparison of path length, direction changes, and runtime across algorithm combinations.}
\label{fig:metrics_comparison}
\end{figure}

\subsubsection{Path Quality and Runtime}

Figure~\ref{fig:metrics_comparison} compares the average path length, number of direction changes, and execution time over successful runs. Overall, \texttt{dwa} produces the shortest trajectories and the lowest execution times, while \texttt{potential\_field} yields the smoothest paths (fewest direction changes). \texttt{evolutionary} achieves high success rates but is substantially slower, reflecting the cost of per-step optimization. In contrast, \texttt{greedy} tends to generate longer and less smooth paths, with \texttt{dijkstra + greedy} exhibiting the largest path lengths and the highest direction-change counts among successful combinations.

\subsubsection{Impact of Obstacle Density}

Figure~\ref{fig:obstacle_impact} shows how increasing the number of moving obstacles affects the top-performing combinations. Overall success decreases as obstacle density increases (from 68.5\% at 0--50 obstacles to 51.2\% at 200 obstacles). Across obstacle counts, \texttt{potential\_field} remains the most stable: it keeps direction-change counts low and execution time nearly flat, with only mild variation in path length. In contrast, \texttt{dwa} maintains short paths but becomes less smooth at high obstacle density (more direction changes), indicating more frequent local avoidance maneuvers. The \texttt{evolutionary} local planner stays reliable but consistently incurs higher execution time than the other top combinations.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{../Code/results/obstacle_impact.png}
\caption{Effect of obstacle count on path length, direction changes, and runtime (top combinations).}
\label{fig:obstacle_impact}
\end{figure}

\subsubsection{Heatmap Summary}

Figure~\ref{fig:heatmap} provides a compact heatmap view of average performance across global and local methods. Consistent with the ranked plots, \texttt{dwa} is best on path length and execution time, \texttt{potential\_field} is best on smoothness, and \texttt{grid\_dfs} tends to underperform other global methods. \texttt{greedy} shows the worst path quality and smoothness, while \texttt{reactive\_dfs} yields no successful runs and is omitted from the heatmaps.

\begin{figure}[ht]
\centering
\includegraphics[width=0.95\linewidth]{../Code/results/algorithm_heatmap.png}
\caption{Heatmap summary of average path length, direction changes, and runtime by method.}
\label{fig:heatmap}
\end{figure}
